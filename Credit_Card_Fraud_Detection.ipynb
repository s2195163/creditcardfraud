{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CUsJX0H3c8ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcZ2QqzCS1WE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/sample_data/creditcard_2023.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUOPxHzvl6Mf"
      },
      "outputs": [],
      "source": [
        "# Split the dataset based on the 'Class' column\n",
        "class_0 = df[df['Class'] == 0]\n",
        "class_1 = df[df['Class'] == 1]\n",
        "\n",
        "# Display the first few rows of each subset to confirm the split\n",
        "print(\"Class 0:\")\n",
        "print(class_0.head())\n",
        "\n",
        "print(\"\\nClass 1:\")\n",
        "print(class_1.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJh8fGHIl-XQ"
      },
      "outputs": [],
      "source": [
        "# Sample 25,000 instances from each class\n",
        "sample_class_0 = class_0.sample(n=25000, replace=True, random_state=42)\n",
        "sample_class_1 = class_1.sample(n=25000, replace=True, random_state=42)\n",
        "\n",
        "# Combine the samples into a single dataframe\n",
        "sampled_df = pd.concat([sample_class_0, sample_class_1])\n",
        "\n",
        "# Shuffle the combined dataframe\n",
        "sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Display the first few rows of the sampled dataframe\n",
        "print(sampled_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHdJAjhsmFGR"
      },
      "outputs": [],
      "source": [
        "# View information about the DataFrame\n",
        "print(sampled_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGM2JtnUUvOx"
      },
      "outputs": [],
      "source": [
        "print('Dataset: ', sampled_df.shape)\n",
        "sampled_df.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32WyMCcMvpLu"
      },
      "outputs": [],
      "source": [
        "sampled_df['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrPGI1SIvwi_"
      },
      "source": [
        "Observations-\n",
        "\n",
        "*   We have 50000 rows of observations having 31 columns.\n",
        "*   Class feature is balanced.\n",
        "*   Class is the feature that indicates whether the transaction is fraudulent (1) or not (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrwtnKDLqq4o"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RC4SifzWFVF"
      },
      "outputs": [],
      "source": [
        "# Checking missing values\n",
        "sns.heatmap(sampled_df.isnull(), cbar=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3x6btvQmXC7"
      },
      "source": [
        "Observations:\n",
        "\n",
        "*   No missing value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBWgIUWvmf-h"
      },
      "outputs": [],
      "source": [
        "duplicates = sampled_df.duplicated(keep=False)\n",
        "duplicate_rows = sampled_df[duplicates]\n",
        "print(duplicate_rows)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Duplicates\n",
        "sampled_df.drop_duplicates(inplace=True)\n",
        "sampled_df['Class'].value_counts()"
      ],
      "metadata": {
        "id": "Hio6fi3CN11t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SaKK313syNU"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU1Nhxyoq1vq"
      },
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "sampled_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkZBxiGQq7Ft"
      },
      "outputs": [],
      "source": [
        "def single_plot_distribution(Class, sampled_df):\n",
        "    # Get the value counts of the specified column\n",
        "    value_counts = sampled_df[Class].value_counts()\n",
        "\n",
        "    # Set up the figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), gridspec_kw={'width_ratios': [1, 1]})\n",
        "\n",
        "    # Donut pie chart\n",
        "    pie_colors = sns.color_palette(\"Set2\", n_colors=3)  # Set2 palette with 3 colors\n",
        "    ax1.pie(value_counts, autopct='%0.001f%%', startangle=90, pctdistance=0.85, colors=pie_colors, labels=None)\n",
        "    centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "    ax1.add_artist(centre_circle)\n",
        "    ax1.set_title(f'Distribution of {Class}', fontsize=16)\n",
        "\n",
        "    # Bar chart\n",
        "    bar_colors = sns.color_palette(\"Set2\", n_colors=len(value_counts))  # Adjusted to the number of unique values\n",
        "    sns.barplot(x=value_counts.index, y=value_counts.values, ax=ax2, palette=bar_colors)\n",
        "    ax2.set_title(f'Count of {Class}', fontsize=16)\n",
        "    ax2.set_xlabel(Class, fontsize=14)\n",
        "    ax2.set_ylabel('Count', fontsize=14)\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Show the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Class Distribution\n",
        "single_plot_distribution('Class', sampled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4eSVgEWIgyr"
      },
      "outputs": [],
      "source": [
        "# Cols to Plot\n",
        "columns_to_plot =['V1', 'V2', 'V3', 'V4','Class']\n",
        "\n",
        "# Data Columns\n",
        "data_to_plot = sampled_df[columns_to_plot]\n",
        "\n",
        "# Create a dictionary to map colors to unique values of the 'Class' column\n",
        "Q_colors = {0: sns.color_palette(\"Set2\")[0], 1: sns.color_palette(\"Set2\")[1]}\n",
        "\n",
        "# Creating the pairplot with the specified palette fo r categorical variables\n",
        "sns.pairplot(data_to_plot, hue='Class', palette=Q_colors)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BxCEoFi7ZTP"
      },
      "outputs": [],
      "source": [
        "# Num_COLS\n",
        "NUM_COLS = sampled_df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Define the number of rows and columns for subplots\n",
        "num_rows = 5  # 4 rows\n",
        "num_cols = 5  # 4 columns\n",
        "\n",
        "# Create subplots with appropriate titles\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 17))\n",
        "\n",
        "# Flatten the axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop through each numerical column and create a box plot\n",
        "for i, col in enumerate(NUM_COLS[:num_rows * num_cols]):\n",
        "    sns.boxplot(x=sampled_df[col], ax=axes[i], color='skyblue')\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "# Hide empty subplots\n",
        "for i in range(len(NUM_COLS), num_rows * num_cols):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK1Sre2f7iHo"
      },
      "outputs": [],
      "source": [
        "# Sample skewness calculation and sorting\n",
        "skewness = sampled_df.skew().sort_values()\n",
        "\n",
        "# Plotting the skewness\n",
        "plt.figure(figsize=(14, 8))  # Adjusted figure size for better visibility\n",
        "barplot = sns.barplot(x=skewness.index, y=skewness.values, palette='viridis')\n",
        "plt.title('Skewness of Features in Sampled DataFrame')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Skewness')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Annotate each bar with its skewness value\n",
        "for i, v in enumerate(skewness.values):\n",
        "    barplot.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=9)  # Adjusted font size\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "* Features like V7, V20 are highly negative skewed.\n",
        "* Features like V5, V28 are highly positive skewed."
      ],
      "metadata": {
        "id": "NaI0UPvUdDU7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJO2OOBu7npG"
      },
      "outputs": [],
      "source": [
        "# Skewed features\n",
        "skewed_features = ['V5', 'V7', 'V20', 'V28']\n",
        "\n",
        "# Create subplots for boxplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Customize colors\n",
        "colors = sns.color_palette(\"Set2\")\n",
        "\n",
        "# Plot boxplots for each skewed feature\n",
        "for i, feature in enumerate(skewed_features):\n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "    sns.boxplot(x=feature, data=df, ax=axes[row, col], color=colors[i])\n",
        "    axes[row, col].set_title(f'Boxplot of {feature}', fontsize=12)\n",
        "    axes[row, col].set_xlabel('')\n",
        "    axes[row, col].set_ylabel('')\n",
        "\n",
        "# Set overall title and adjust layout\n",
        "plt.suptitle('Boxplots of Skewed Features', size=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igIPsRZzrOli"
      },
      "outputs": [],
      "source": [
        "# Plot correlation matrix\n",
        "corr_df = sampled_df.corr(method = 'spearman').round(2)\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
        "plt.figure(figsize=[25,15])\n",
        "sns.heatmap(corr_df, cmap=sns.color_palette(\"magma\", as_cmap = True), annot=True, mask = mask)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHXqxXZ_sCH_"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l4NN3YWhRe-"
      },
      "outputs": [],
      "source": [
        "# Drop the target variable ('Class') before calculating VIF\n",
        "independent_variables = sampled_df.drop(columns=['Class'])\n",
        "\n",
        "# Calculate VIF for each independent variable\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = independent_variables.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(independent_variables.values, i) for i in range(independent_variables.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVo7vYRKiFPi"
      },
      "outputs": [],
      "source": [
        "# Sort the VIF data for better visualization\n",
        "sorted_vif_data = vif_data.sort_values(by='VIF', ascending=False)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(sorted_vif_data['Variable'], sorted_vif_data['VIF'], color='skyblue')\n",
        "plt.xlabel('VIF')\n",
        "plt.ylabel('Variable')\n",
        "plt.title('VIF Values')\n",
        "plt.grid(axis='x')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations:\n",
        "\n",
        "* Variables like 'V17' and 'V16' have particularly high VIF values.\n",
        "* Multicollinearity exists."
      ],
      "metadata": {
        "id": "Osv72njpO_F4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY94Us7g2_9j"
      },
      "source": [
        "### Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XUiDXY-CbNV"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYW6PLABCeet"
      },
      "outputs": [],
      "source": [
        "# Drop the 'id' column from the DataFrame\n",
        "sampled_df_without_id = sampled_df.drop(columns=['id'])\n",
        "\n",
        "# Extract features from DataFrame without 'id'\n",
        "X = sampled_df_without_id.values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize PCA\n",
        "pca = PCA()\n",
        "\n",
        "# Fit PCA to the standardized features\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Determine the number of principal components to retain\n",
        "explained_variance_ratio_cumulative = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components_to_retain = np.argmax(explained_variance_ratio_cumulative >= 0.95) + 1\n",
        "\n",
        "# Apply PCA transformation to the standardized features\n",
        "pca = PCA(n_components=num_components_to_retain)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Convert X_pca to a DataFrame\n",
        "X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i}' for i in range(1, num_components_to_retain+1)])\n",
        "X_pca_df.head()\n",
        "\n",
        "# Concatenate X_pca_df with the original DataFrame sampled_df_without_id\n",
        "standardized_data_df = pd.concat([X_pca_df, sampled_df_without_id], axis=1)\n",
        "standardized_data_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl_QtOd_ptV9"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvsRLGp7A0M3"
      },
      "source": [
        "Features Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0PezT4Q9rZk"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHiK7H5PPPn2"
      },
      "outputs": [],
      "source": [
        "x_data = df.iloc[:, 1:-1] # characteristic value\n",
        "y_data = df.iloc[:, -1] # labels\n",
        "# Partition dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=42)\n",
        "# Using ANOVA F-value as the scoring function to select the best 10 features\n",
        "selector = SelectKBest(f_classif, k=10)\n",
        "selector.fit(x_train, y_train)\n",
        "# Obtain the index of the selected features\n",
        "selected_features_indices = selector.get_support(indices=True)\n",
        "# Obtain the name of the selected feature\n",
        "selected_features_names = x_data.columns[selected_features_indices]\n",
        "# Print selected feature names\n",
        "print(\"Selected features:\", selected_features_names)\n",
        "# Draw rating charts for features\n",
        "plt.bar(range(len(selector.scores_)), selector.scores_)\n",
        "plt.xticks(range(len(selector.scores_)), x_data.columns, rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd0MIYrHCUz8"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target variable\n",
        "\n",
        "X = df[['V1', 'V2', 'V3', 'V4', 'V9', 'V10', 'V11', 'V12', 'V14', 'V16']]\n",
        "y = df['Class']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "X_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "hg7NPD3bGjr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7c4XzWtp4mx"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1tyIfULEFA0"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah67lWpspqJI"
      },
      "outputs": [],
      "source": [
        "# Train a Random Forest classifier\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "RandomForest_accuracy = accuracy_score(y_test, y_pred)\n",
        "RandomForest_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "RandomForest_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {RandomForest_accuracy:.2f}')\n",
        "print(f'Confusion Matrix:\\n{RandomForest_conf_matrix}')\n",
        "print(f'Classification Report:\\n{RandomForest_report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtQiGrnnp_qR"
      },
      "source": [
        "Logistic Regression Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRIHxz8HFnpQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KchDkBSxqAQy"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Analysis\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "LogisticRegression_accuracy = accuracy_score(y_test, y_pred)\n",
        "LogisticRegression_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "LogisticRegression_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {LogisticRegression_accuracy:.2f}')\n",
        "print(f'Confusion Matrix:\\n{LogisticRegression_conf_matrix}')\n",
        "print(f'Classification Report:\\n{LogisticRegression_report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovc1BmsWqxUJ"
      },
      "source": [
        "Support Vector Classifier (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI6DmfBqFvPh"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cosrhbbwq3Oj"
      },
      "outputs": [],
      "source": [
        "# Support Vector Classifier (SVC) model\n",
        "svc_model = SVC(kernel='linear')\n",
        "\n",
        "# Train the SVC model\n",
        "svc_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svc_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "svc_accuracy = accuracy_score(y_test, y_pred)\n",
        "svc_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "svc_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {svc_accuracy:.2f}')\n",
        "print(f'Confusion Matrix:\\n{svc_conf_matrix}')\n",
        "print(f'Classification Report:\\n{svc_report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTxq6kv3q5IL"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 此内容为代码格式\n",
        "```\n",
        "\n",
        "XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-9GhYGjq8F4"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# XGBoost Classifier model\n",
        "xgb_model = XGBClassifier()\n",
        "\n",
        "# Train the XGBoost Classifier model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "xgb_accuracy = accuracy_score(y_test, y_pred)\n",
        "xgb_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "xgb_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {xgb_accuracy:.2f}')\n",
        "print(f'Confusion Matrix:\\n{xgb_conf_matrix}')\n",
        "print(f'Classification Report:\\n{xgb_report}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vrqs4IsUiH8"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary with evaluation results\n",
        "evaluation_data = {\n",
        "    'Model': ['RandomForest', 'LogisticRegression', 'svc', 'xgb'],\n",
        "    'Accuracy': [RandomForest_accuracy, LogisticRegression_accuracy, svc_accuracy,xgb_accuracy],\n",
        "}\n",
        "\n",
        "# Create a dataframe\n",
        "evaluation_df = pd.DataFrame(evaluation_data)\n",
        "\n",
        "# Sort the dataframe based on Accuracy and Precision columns in descending order\n",
        "evaluation_df = evaluation_df.sort_values(by=['Accuracy'], ascending=False)\n",
        "\n",
        "# Display the sorted dataframe\n",
        "evaluation_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujQcEL76h9Kv"
      },
      "source": [
        "#Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l-gx7B3YBLE"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save model artifact to local filesystem\n",
        "# Save XGBoost model as xgboost.pkl\n",
        "with open('xgboost.pkl', 'wb') as model_file:\n",
        "  pickle.dump(xgb_model, model_file)\n",
        "\n",
        "# Save Random Forest model as randomforest.pkl\n",
        "with open('randomforest.pkl', 'wb') as model_file:\n",
        "  pickle.dump(classifier, model_file)\n",
        "\n",
        "# Save Support Vector Classifier model as supportvector.pkl\n",
        "with open('supportvector.pkl', 'wb') as model_file:\n",
        "  pickle.dump(svc_model, model_file)\n",
        "\n",
        "# Save Logistic Regression model as logisticregression.pkl\n",
        "with open('logisticregression.pkl', 'wb') as model_file:\n",
        "  pickle.dump(log_model, model_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kZw-_Z4B8ckz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}